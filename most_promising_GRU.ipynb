{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6ef6f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16dfcb1",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8a3fbcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad to len: 114\n"
     ]
    }
   ],
   "source": [
    "import selfies as sf\n",
    "data = pd.read_csv('./GRU_data/selfies.csv', header=None, names=['selfies'])\n",
    "alphabet = sf.get_alphabet_from_selfies(data.selfies)\n",
    "alphabet.add(\"[nop]\") # [nop] is a special padding symbol\n",
    "alphabet.add(\"[start]\")\n",
    "alphabet.add(\"[end]\")\n",
    "alphabet = list(sorted(alphabet))\n",
    "pad_to_len = max(sf.len_selfies(s) for s in data.selfies) + 5\n",
    "print(\"Pad to len:\", pad_to_len)\n",
    "symbol_to_idx = {s: i for i, s in enumerate(alphabet)}\n",
    "idx2char = {i: s for i, s in enumerate(alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "72177872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3ca0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GRUDataset(Dataset):\n",
    "    def __init__(self, smiles_fp, selfies, vectorizer):\n",
    "        self.smiles_fp = pd.read_csv(smiles_fp, sep=',', nrows=10000)\n",
    "        self.selfies = pd.read_csv(selfies, nrows=10000)\n",
    "        self.X = self.prepare_X(self.smiles_fp)\n",
    "        self.X = np.array([self.reconstruct_fp(fp) for fp in self.X])\n",
    "        self.y = self.prepare_y(self.selfies)\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_fp)\n",
    "    def __getitem__(self, idx):\n",
    "        raw_selfie = self.y[idx][0]\n",
    "        vectorized_selfie = vectorizer.vectorize(raw_selfie)\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.from_numpy(vectorized_selfie).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_X(smiles_fp):\n",
    "        fps = smiles_fp.fps.apply(eval).apply(lambda x: np.array(x, dtype=int))\n",
    "        return fps\n",
    "    @staticmethod\n",
    "    def prepare_y(selfies):\n",
    "        return selfies.values\n",
    "    @staticmethod\n",
    "    def reconstruct_fp(fp, length=4860):\n",
    "        fp_rec = np.zeros(length)\n",
    "        fp_rec[fp] = 1\n",
    "        return fp_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f55a6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class SELFIESVectorizer:\n",
    "    def __init__(self, alphabet, pad_to_len):\n",
    "        self.alphabet = alphabet\n",
    "        self.pad_to_len = pad_to_len\n",
    "        self.char2idx = {s: i for i, s in enumerate(alphabet)}\n",
    "        self.idx2char = {i: s for i, s in enumerate(alphabet)}\n",
    "    def vectorize(self, selfie):\n",
    "        ''' Vectorize a list of SMILES strings to a numpy array of shape (len(smiles), embed, len(charset))'''\n",
    "        X = np.zeros((self.pad_to_len, len(self.alphabet)))\n",
    "        splited = ['[start]'] + self.split_selfi(selfie) + ['[end]'] + ['[nop]'] * (self.pad_to_len - len(self.split_selfi(selfie)) - 2)\n",
    "        for i, char in enumerate(splited):\n",
    "            X[i, self.char2idx[char]] = 1\n",
    "        return X\n",
    "    def devectorize(self, ohe):\n",
    "        ''' Devectorize a numpy array of shape (len(smiles), embed, len(charset)) to a list of SMILES strings'''\n",
    "        selfie_str = ''\n",
    "        for j in range(self.pad_to_len):\n",
    "            char = self.idx2char[np.argmax(ohe[j])]\n",
    "            if char == '[start]':\n",
    "                continue\n",
    "            elif char == '[end]':\n",
    "                break\n",
    "            else:\n",
    "                selfie_str += char\n",
    "        return selfie_str\n",
    "\n",
    "    def split_selfi(self, selfie):\n",
    "        pattern = r'(\\[[^\\[\\]]*\\])'\n",
    "        return re.findall(pattern, selfie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "70924816",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = SELFIESVectorizer(alphabet, pad_to_len)\n",
    "dataset = GRUDataset('GRU_data/chembl_klek.csv', 'GRU_data/selfies.csv', vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "062168b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "da4f014e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4860])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f39daf",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8248e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class EncoderNet(nn.Module):\n",
    "    def __init__(self, fp_size, encoding_size):\n",
    "        super(EncoderNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(fp_size, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, encoding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.relu(self.fc5(out))\n",
    "        return out\n",
    "\n",
    "class DecoderNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, drop_prob):\n",
    "        super(DecoderNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.max_len = vectorizer.pad_to_len\n",
    "\n",
    "    def forward(self, encoded):\n",
    "        batch_size = encoded.size(0)\n",
    "        hidden = self.init_hidden(encoded)\n",
    "        start_vector = vectorizer.vectorize('[start]')\n",
    "        start_vector = torch.from_numpy(start_vector).float().to(device)\n",
    "        start_vector = start_vector.unsqueeze(0)\n",
    "        start_vector = start_vector.repeat(batch_size, 1, 1)\n",
    "        print(f'Start vector size: {start_vector.size()}')\n",
    "        decoded, hidden = self.gru(start_vector, hidden)\n",
    "        decoded = self.fc(decoded)\n",
    "        return decoded\n",
    "\n",
    "    def init_hidden(self, encoded):\n",
    "        \n",
    "        return encoded.unsqueeze(0).repeat(self.num_layers, 1, 1).to(device)\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=4860, encoding_size=128, decoding_size=128, output_size=42, num_layers=2, drop_prob=0.1):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = EncoderNet(input_size, encoding_size)\n",
    "        self.decoder = DecoderNet(output_size, decoding_size, output_size, num_layers, drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1f37f3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderNet(4860, 128).to(device)\n",
    "decoder = DecoderNet(42, 128, 42, 2, 0.1).to(device)\n",
    "test_batch = next(iter(train_loader))[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5ab9b122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test batch shape: torch.Size([64, 4860])\n",
      "Encoded shape: torch.Size([64, 128])\n",
      "Encoded: tensor([0.0000, 0.0000, 0.0473, 0.0000, 0.0000, 0.0238, 0.0358, 0.0000, 0.0000,\n",
      "        0.0353, 0.0000, 0.0444, 0.0000, 0.0371, 0.0310, 0.0302, 0.0237, 0.0000,\n",
      "        0.0000, 0.0000, 0.0088, 0.0000, 0.0000, 0.0058, 0.0267, 0.0532, 0.0038,\n",
      "        0.0265, 0.0000, 0.0462, 0.0000, 0.0000, 0.0364, 0.0000, 0.0197, 0.0000,\n",
      "        0.0000, 0.0521, 0.0534, 0.0008, 0.0240, 0.0000, 0.0000, 0.0489, 0.0074,\n",
      "        0.0000, 0.0116, 0.0137, 0.0000, 0.0000, 0.0216, 0.0000, 0.0515, 0.0596,\n",
      "        0.0054, 0.0000, 0.0410, 0.0017, 0.0090, 0.0000, 0.0150, 0.0000, 0.0000,\n",
      "        0.0000, 0.0518, 0.0000, 0.0000, 0.0519, 0.0627, 0.0000, 0.0000, 0.0096,\n",
      "        0.0472, 0.0014, 0.0000, 0.0349, 0.0198, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0696, 0.0000, 0.0004, 0.0000, 0.0000, 0.0048, 0.0000, 0.0340, 0.0257,\n",
      "        0.0000, 0.0000, 0.0044, 0.0203, 0.0639, 0.0023, 0.0126, 0.0500, 0.0164,\n",
      "        0.0402, 0.0043, 0.0000, 0.0312, 0.0000, 0.0000, 0.0000, 0.0595, 0.0224,\n",
      "        0.0516, 0.0000, 0.0163, 0.0419, 0.0000, 0.0000, 0.0000, 0.0534, 0.0000,\n",
      "        0.0000, 0.0000, 0.0089, 0.0495, 0.0616, 0.0000, 0.0274, 0.0007, 0.0000,\n",
      "        0.0224, 0.0416], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'Test batch shape: {test_batch.shape}')\n",
    "encoded = encoder(test_batch)\n",
    "print(f'Encoded shape: {encoded.shape}')\n",
    "print(f'Encoded: {encoded[0, :128]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "54cfdfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded shape: torch.Size([64, 114, 42])\n"
     ]
    }
   ],
   "source": [
    "decoded = decoder(encoded)\n",
    "print(f'Decoded shape: {decoded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a208610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(4860, 300, 300, 42, 2, 0.0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f2893d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start vector size: torch.Size([64, 114, 42])\n",
      "Encoded shape: torch.Size([64, 300])\n",
      "Decoded shape: torch.Size([64, 114, 42])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    encoded, decoded = model(test_batch)\n",
    "    print(f'Encoded shape: {encoded.shape}')\n",
    "    print(f'Decoded shape: {decoded.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f697df",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "36f20c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(autoencoder, dataloader, num_epochs=10, device=device):\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    print('Training started')\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            encoded, decoded = autoencoder(x)\n",
    "            loss = criterion(decoded, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8ce62391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_size = 4860\n",
    "# encoding_size = 128\n",
    "# decoding_size = 128\n",
    "# output_size = 42\n",
    "# num_layers = 2\n",
    "\n",
    "# model = Autoencoder(input_size=input_size, encoding_size=encoding_size, decoding_size=decoding_size,\n",
    "#                     output_size=output_size, num_layers=num_layers, drop_prob=0.2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8650a0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "Epoch [1/200], Loss: 11.2717\n",
      "Epoch [2/200], Loss: 11.2190\n",
      "Epoch [3/200], Loss: 11.2220\n",
      "Epoch [4/200], Loss: 11.2089\n",
      "Epoch [5/200], Loss: 11.2127\n",
      "Epoch [6/200], Loss: 11.1438\n",
      "Epoch [7/200], Loss: 11.2166\n",
      "Epoch [8/200], Loss: 11.2059\n",
      "Epoch [9/200], Loss: 11.1810\n",
      "Epoch [10/200], Loss: 11.2048\n",
      "Epoch [11/200], Loss: 11.1278\n",
      "Epoch [12/200], Loss: 11.1394\n",
      "Epoch [13/200], Loss: 11.1911\n",
      "Epoch [14/200], Loss: 11.1235\n",
      "Epoch [15/200], Loss: 11.1235\n",
      "Epoch [16/200], Loss: 11.1210\n",
      "Epoch [17/200], Loss: 11.1605\n",
      "Epoch [18/200], Loss: 11.0968\n",
      "Epoch [19/200], Loss: 11.1586\n",
      "Epoch [20/200], Loss: 11.1293\n",
      "Epoch [21/200], Loss: 11.1211\n",
      "Epoch [22/200], Loss: 11.1406\n",
      "Epoch [23/200], Loss: 11.1354\n",
      "Epoch [24/200], Loss: 11.1209\n",
      "Epoch [25/200], Loss: 11.1470\n",
      "Epoch [26/200], Loss: 11.1504\n",
      "Epoch [27/200], Loss: 11.0867\n",
      "Epoch [28/200], Loss: 11.0608\n",
      "Epoch [29/200], Loss: 11.0918\n",
      "Epoch [30/200], Loss: 11.0747\n",
      "Epoch [31/200], Loss: 11.1075\n",
      "Epoch [32/200], Loss: 11.0803\n",
      "Epoch [33/200], Loss: 11.1330\n",
      "Epoch [34/200], Loss: 11.0234\n",
      "Epoch [35/200], Loss: 11.0516\n",
      "Epoch [36/200], Loss: 11.0189\n",
      "Epoch [37/200], Loss: 11.0483\n",
      "Epoch [38/200], Loss: 11.0215\n",
      "Epoch [39/200], Loss: 11.0696\n",
      "Epoch [40/200], Loss: 11.0190\n",
      "Epoch [41/200], Loss: 11.0437\n",
      "Epoch [42/200], Loss: 11.0003\n",
      "Epoch [43/200], Loss: 11.0664\n",
      "Epoch [44/200], Loss: 11.0217\n",
      "Epoch [45/200], Loss: 10.9075\n",
      "Epoch [46/200], Loss: 10.9781\n",
      "Epoch [47/200], Loss: 10.9945\n",
      "Epoch [48/200], Loss: 11.0257\n",
      "Epoch [49/200], Loss: 11.0113\n",
      "Epoch [50/200], Loss: 10.9840\n",
      "Epoch [51/200], Loss: 10.9994\n",
      "Epoch [52/200], Loss: 10.9959\n",
      "Epoch [53/200], Loss: 10.9841\n",
      "Epoch [54/200], Loss: 10.9742\n",
      "Epoch [55/200], Loss: 10.9735\n",
      "Epoch [56/200], Loss: 11.0156\n",
      "Epoch [57/200], Loss: 10.9704\n",
      "Epoch [58/200], Loss: 10.9666\n",
      "Epoch [59/200], Loss: 10.9369\n",
      "Epoch [60/200], Loss: 10.9834\n",
      "Epoch [61/200], Loss: 10.9557\n",
      "Epoch [62/200], Loss: 11.0422\n",
      "Epoch [63/200], Loss: 10.9805\n",
      "Epoch [64/200], Loss: 10.9948\n",
      "Epoch [65/200], Loss: 10.9862\n",
      "Epoch [66/200], Loss: 10.9446\n",
      "Epoch [67/200], Loss: 10.9366\n",
      "Epoch [68/200], Loss: 11.0036\n",
      "Epoch [69/200], Loss: 10.9673\n",
      "Epoch [70/200], Loss: 10.9825\n",
      "Epoch [71/200], Loss: 10.9586\n",
      "Epoch [72/200], Loss: 10.9670\n",
      "Epoch [73/200], Loss: 10.9879\n",
      "Epoch [74/200], Loss: 10.9949\n",
      "Epoch [75/200], Loss: 10.9846\n",
      "Epoch [76/200], Loss: 10.9409\n",
      "Epoch [77/200], Loss: 10.9600\n",
      "Epoch [78/200], Loss: 10.9575\n",
      "Epoch [79/200], Loss: 10.9608\n",
      "Epoch [80/200], Loss: 10.9724\n",
      "Epoch [81/200], Loss: 10.9374\n",
      "Epoch [82/200], Loss: 10.9511\n",
      "Epoch [83/200], Loss: 10.9444\n",
      "Epoch [84/200], Loss: 10.9842\n",
      "Epoch [85/200], Loss: 10.9658\n",
      "Epoch [86/200], Loss: 10.9445\n",
      "Epoch [87/200], Loss: 10.9945\n",
      "Epoch [88/200], Loss: 10.9568\n",
      "Epoch [89/200], Loss: 10.9398\n",
      "Epoch [90/200], Loss: 10.9060\n",
      "Epoch [91/200], Loss: 10.9217\n",
      "Epoch [92/200], Loss: 10.9229\n",
      "Epoch [93/200], Loss: 10.9351\n",
      "Epoch [94/200], Loss: 10.9868\n",
      "Epoch [95/200], Loss: 10.9442\n",
      "Epoch [96/200], Loss: 10.9910\n",
      "Epoch [97/200], Loss: 10.9271\n",
      "Epoch [98/200], Loss: 10.9450\n",
      "Epoch [99/200], Loss: 10.9633\n",
      "Epoch [100/200], Loss: 10.9947\n",
      "Epoch [101/200], Loss: 11.0014\n",
      "Epoch [102/200], Loss: 10.9195\n",
      "Epoch [103/200], Loss: 10.9510\n",
      "Epoch [104/200], Loss: 10.9241\n",
      "Epoch [105/200], Loss: 10.9328\n",
      "Epoch [106/200], Loss: 10.9705\n",
      "Epoch [107/200], Loss: 10.9478\n",
      "Epoch [108/200], Loss: 10.9270\n",
      "Epoch [109/200], Loss: 10.9606\n",
      "Epoch [110/200], Loss: 10.9613\n",
      "Epoch [111/200], Loss: 10.9042\n",
      "Epoch [112/200], Loss: 10.9180\n",
      "Epoch [113/200], Loss: 10.9614\n",
      "Epoch [114/200], Loss: 10.8459\n",
      "Epoch [115/200], Loss: 10.9272\n",
      "Epoch [116/200], Loss: 10.9192\n",
      "Epoch [117/200], Loss: 10.9052\n",
      "Epoch [118/200], Loss: 10.9213\n",
      "Epoch [119/200], Loss: 10.9252\n",
      "Epoch [120/200], Loss: 10.9823\n",
      "Epoch [121/200], Loss: 10.9209\n",
      "Epoch [122/200], Loss: 10.9329\n",
      "Epoch [123/200], Loss: 10.9187\n",
      "Epoch [124/200], Loss: 10.8867\n",
      "Epoch [125/200], Loss: 10.9541\n",
      "Epoch [126/200], Loss: 10.9153\n",
      "Epoch [127/200], Loss: 10.9169\n",
      "Epoch [128/200], Loss: 10.9383\n",
      "Epoch [129/200], Loss: 10.9319\n",
      "Epoch [130/200], Loss: 10.9439\n",
      "Epoch [131/200], Loss: 10.8553\n",
      "Epoch [132/200], Loss: 10.9059\n",
      "Epoch [133/200], Loss: 10.9667\n",
      "Epoch [134/200], Loss: 10.9793\n",
      "Epoch [135/200], Loss: 10.8560\n",
      "Epoch [136/200], Loss: 10.8616\n",
      "Epoch [137/200], Loss: 10.9193\n",
      "Epoch [138/200], Loss: 10.8832\n",
      "Epoch [139/200], Loss: 10.9130\n",
      "Epoch [140/200], Loss: 10.8820\n",
      "Epoch [141/200], Loss: 10.9159\n",
      "Epoch [142/200], Loss: 10.8872\n",
      "Epoch [143/200], Loss: 10.9279\n",
      "Epoch [144/200], Loss: 10.8881\n",
      "Epoch [145/200], Loss: 10.9322\n",
      "Epoch [146/200], Loss: 10.9168\n",
      "Epoch [147/200], Loss: 10.9214\n",
      "Epoch [148/200], Loss: 10.9224\n",
      "Epoch [149/200], Loss: 10.8960\n",
      "Epoch [150/200], Loss: 10.9138\n",
      "Epoch [151/200], Loss: 10.9547\n",
      "Epoch [152/200], Loss: 10.8522\n",
      "Epoch [153/200], Loss: 10.9501\n",
      "Epoch [154/200], Loss: 10.8694\n",
      "Epoch [155/200], Loss: 10.8900\n",
      "Epoch [156/200], Loss: 10.9060\n",
      "Epoch [157/200], Loss: 10.8834\n",
      "Epoch [158/200], Loss: 10.8193\n",
      "Epoch [159/200], Loss: 10.8642\n",
      "Epoch [160/200], Loss: 10.8867\n",
      "Epoch [161/200], Loss: 10.8555\n",
      "Epoch [162/200], Loss: 10.8726\n",
      "Epoch [163/200], Loss: 10.8175\n",
      "Epoch [164/200], Loss: 10.8762\n",
      "Epoch [165/200], Loss: 10.9072\n",
      "Epoch [166/200], Loss: 10.8921\n",
      "Epoch [167/200], Loss: 10.8921\n",
      "Epoch [168/200], Loss: 10.8698\n",
      "Epoch [169/200], Loss: 10.9577\n",
      "Epoch [170/200], Loss: 10.8433\n",
      "Epoch [171/200], Loss: 10.8499\n",
      "Epoch [172/200], Loss: 10.8191\n",
      "Epoch [173/200], Loss: 10.8385\n",
      "Epoch [174/200], Loss: 10.8894\n",
      "Epoch [175/200], Loss: 10.8691\n",
      "Epoch [176/200], Loss: 10.8765\n",
      "Epoch [177/200], Loss: 10.8563\n",
      "Epoch [178/200], Loss: 10.8708\n",
      "Epoch [179/200], Loss: 10.8854\n",
      "Epoch [180/200], Loss: 10.8842\n",
      "Epoch [181/200], Loss: 10.8503\n",
      "Epoch [182/200], Loss: 10.8313\n",
      "Epoch [183/200], Loss: 10.8765\n",
      "Epoch [184/200], Loss: 10.8223\n",
      "Epoch [185/200], Loss: 10.8093\n",
      "Epoch [186/200], Loss: 10.8349\n",
      "Epoch [187/200], Loss: 10.7935\n",
      "Epoch [188/200], Loss: 10.8170\n",
      "Epoch [189/200], Loss: 10.8576\n",
      "Epoch [190/200], Loss: 10.8506\n",
      "Epoch [191/200], Loss: 10.8314\n",
      "Epoch [192/200], Loss: 10.8780\n",
      "Epoch [193/200], Loss: 11.0160\n",
      "Epoch [194/200], Loss: 11.0177\n",
      "Epoch [195/200], Loss: 10.9596\n",
      "Epoch [196/200], Loss: 10.8975\n",
      "Epoch [197/200], Loss: 10.8778\n",
      "Epoch [198/200], Loss: 10.8484\n",
      "Epoch [199/200], Loss: 10.8561\n",
      "Epoch [200/200], Loss: 10.8786\n"
     ]
    }
   ],
   "source": [
    "model = train(autoencoder=model, dataloader=train_loader, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8614d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model just in case\n",
    "torch.save(model.state_dict(), './GRU_data/model_200e.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2bad90c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hubert jakby co, to możesz wczytać model z pliku, \n",
    "# żeby nie trzeba było czekać 200 epok\n",
    "loaded_model = Autoencoder(4860, 300, 300, 42, 2, 0.0).to(device)\n",
    "loaded_model.load_state_dict(torch.load('./GRU_data/model_200e.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2e768d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4860])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x = x.to(device)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9dbdfd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start vector size: torch.Size([64, 114, 42])\n"
     ]
    }
   ],
   "source": [
    "encoded, decoded = loaded_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe6bfa9",
   "metadata": {},
   "source": [
    "# GRU output to SELFIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2aab1202",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_indices = torch.argmax(decoded.cpu(), dim=2)\n",
    "decoded_indices = decoded_indices.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ba9c451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bad version\n",
    "# selfies = []\n",
    "# for i in decoded_indices:\n",
    "#     vectorized = []\n",
    "#     for number in decoded_indices[0]:\n",
    "#         v = np.zeros(128)\n",
    "#         v[number] = 1\n",
    "#         vectorized.append(v)\n",
    "#     devectorized = vectorizer.devectorize(vectorized)\n",
    "#     selfies.append(devectorized)\n",
    "# selfies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c4a1d92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [C][C][=N][C][Branch2][Ring2][=Branch1][C][=C][C][=C][Branch2][Ring1][=Branch2][C][=C][C][=C][C][=C][Ring1][=Branch1][S][=Branch1][C][=O][=Branch1][C][=O][N][C][Branch1][C][C][Branch1][C][C][C][C][=C][Ring2][Ring1][Ring2][F][=C][N][=C][Ring2][Ring1][O][N] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][Branch1][C][C][C@H1][C][C][=C][Branch1][C][O][C][=C][C][=C][Ring1][#Branch1][C@][Ring1][N][Branch1][C][C][C][C][N][Ring1][=C][C][=Branch1][C][=O][C@@H1][C][C][C][Branch1][=C][N][S][=Branch1][C][=O][=Branch1][C][=O][C][C][C][Ring1][Ring1][C][Ring1][N] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][=C][C][=C][Branch1][=C][/C][=Branch1][Ring1][=N][/O][N][C][C][O][C][C][Ring1][=Branch1][C][Branch1][S][O][C][=C][C][=C][C][=C][C][=C][C][Ring1][=Branch1][=C][Ring1][#Branch2][=N][Ring2][Ring1][#Branch2] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][N][N][=C][C][Branch2][Ring1][=N][N][C][=Branch1][C][=O][C][N][=C][Branch1][=C][C][=C][C][=C][C][=C][Ring1][=Branch1][C][Branch1][C][F][F][S][C][=Ring1][=C][N][=C][Ring2][Ring1][#Branch1][C@@H1][C][C][C@@H1][Branch1][C][N][C@H1][Branch1][C][F][C][O][Ring1][=Branch2] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][Branch1][C][C][C@H1][C][O][C][=Branch1][C][=O][N][Ring1][=Branch1][C][=C][C][=N][C][Branch2][Ring1][Branch2][N][C@@H1][Branch1][C][C][C][=C][C][=C][Branch1][=Branch2][N][C][C][O][C][C][Ring1][=Branch1][C][=C][Ring1][N][=N][Ring2][Ring1][Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][=C][C][=C][Branch1][S][S][=Branch1][C][=O][=Branch1][C][=O][N][C][C][C][C][C][Ring1][=Branch1][C][=C][Ring1][#C][N][C][=Branch1][C][=O][C][S][C][=N][C][Branch1][C][N][=C][C][Branch1][C][O][=N][Ring1][Branch2] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][C][=C][Branch2][Ring1][#C][N][C][=N][C][C][Branch1][=C][C][=Branch1][C][=O][N][C][Branch1][C][C][Branch1][C][C][C][=C][NH1][C][=Ring1][N][N][=C][Ring1][S][Cl][S][N][=Ring2][Ring1][#Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][O][C][=Branch1][C][=O][C][N][C][=C][Branch2][Ring1][=Branch1][C][=Branch1][C][=O][C@@H1][C][S][C][Branch1][=Branch2][C][=C][C][=C][N][=C][Ring1][=Branch1][N][Ring1][O][C][=C][C][=C][C][=C][Ring1][=Branch1][Ring2][Ring1][=Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][O][C][=Branch1][C][=O][C][=C][O][C@H1][Branch1][C][C][C@@H1][C][N][C][C][C][=C][Branch1][S][NH1][C][=C][C][Branch1][Ring1][O][C][=C][C][=C][Ring1][O][Ring1][Branch2][C@@H1][Ring1][#C][C][C@H1][Ring2][Ring1][Branch2][Ring2][Ring1][Ring1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [O][=C][Branch2][Ring1][N][N][C][C][C][C][N][C][=Branch1][C][=O][C][C][=C][Branch1][=Branch2][C][=C][C][=C][C][=C][Ring1][=Branch1][O][N][=Ring1][O][C][=C][S][C][=N][Ring1][Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][Branch1][C][C][C][N][=C][Branch1][=C][N][C][=Branch1][C][=O][C][=C][C][=C][C][=N][Ring1][=Branch1][S][C][=Ring1][=C][S][=Branch1][C][=O][=Branch1][C][=O][C][C][=C][C][=C][Branch1][C][Cl][C][=C][Ring1][#Branch1][F] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][C][=C][Branch1][C][C][N][C][=C][Branch2][Ring1][=C][C][S][C][=N][C][Branch1][=Branch2][C][=C][C][=C][C][=C][Ring1][=Branch1][=C][N][Ring1][O][C][C][N][C][C][O][C][C][Ring1][=Branch1][N][=C][Ring2][Ring1][#Branch2][N][=Ring2][Ring1][#C] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][C][C][O][C][=C][C][Branch1][#Branch1][N][C][Branch1][C][C][=O][=C][Branch1][C][N][C][=C][Ring1][O][C][=Branch1][C][=O][O][C] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][=C][C][=C][Branch1][N][C][C][Branch1][Ring1][C][S][C][=Branch1][C][=O][O][C][=C][Ring1][=N][C] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][O][C][=C][Branch1][C][C][C][O][C][=Branch1][C][=O][C][=C][Branch1][C][C][C][=Ring1][Branch2][C][=C][Ring1][=N][C][=Ring1][S][C][N][C][C][O] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][C][C][O][C][=C][C][=C][Branch1][P][C][N][C][=Branch1][C][=O][O][C][=C][C][=C][C][=C][Ring1][=Branch1][F][C][=C][Ring2][Ring1][C] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][O][N][=C][Branch1][=Branch2][C][=C][C][=C][C][=C][Ring1][=Branch1][C][=Ring1][O][C][O][C][=C][C][=C][Branch1][N][C][=Branch1][C][=O][N][C][C][Branch1][C][F][F][C][=N][Ring1][=N] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][=C][C][Branch2][Branch1][Ring1][C][=N][NH1][C][=C][C][=C][Branch2][Ring1][P][C][=Branch1][C][=O][N][N][C][C][C][C][Branch1][Ring1][C][O][Branch1][=C][O][C][=C][C][Branch1][C][F][=C][C][=C][Ring1][#Branch1][F][C][Ring1][P][C][=C][Ring2][Ring1][=N][Ring2][Ring1][#Branch2][=C][C][=N][Ring2][Ring2][Ring1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][Branch1][C][C][C][=C][C][=C][C][=C][Ring1][=Branch1][N][C][=Branch1][C][=O][C][=C][C][=C][C][Branch2][Ring1][Ring1][C][C][=N][NH1][C][=Branch1][C][=O][C][=C][Ring1][#Branch1][C][C][C][C][Ring1][=Branch1][=C][Ring2][Ring1][C] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][C][=Branch1][C][=O][N][C][C][C][=C][Branch1][Branch2][O][C][=Branch1][C][=O][C][Br][C][=C][C][=C][C][=C][Branch1][Ring1][O][C][C][=C][Ring1][P][Ring1][Branch2] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][=C][C][=C][C][=C][Ring1][=Branch1][S][=Branch1][C][=O][=Branch1][C][=O][N][C][=Branch1][C][=O][C][Branch1][C][C][C][C][N][Ring1][Branch1][C][=Branch1][C][=O][C][C][C][=C][C][=C][S][Ring1][Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [O][=C][C][C][C][C@][Branch1][N][C][=C][C][=C][Branch1][C][Cl][C][=C][Ring1][#Branch1][S][C][C][N][Ring1][S][Ring1][N] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][C][=C][C][=Branch1][C][=O][N][Branch2][Ring1][Branch2][C][=C][C][=C][Branch1][=C][O][C][C][C][C][C][N][C][C][C][C][Ring1][Branch1][C][=C][Ring1][P][C][=Ring2][Ring1][Branch2] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C@H1][C][=C][N][C][C][C@@H1][Branch1][C][O][C][Ring1][#Branch1][C@H1][Branch1][P][C][C][=Branch1][C][=O][C@H1][Ring1][=Branch1][C][C@H1][Branch1][C][O][C][Ring1][#C][C][Ring2][Ring1][Ring1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [Br][Br][Br][C][C][C][N][C][C][N][Branch2][Ring1][=C][C][=N][C][Branch2][Ring1][Ring1][C][C][N][Branch1][C][C][C][C][C][C][C][=C][C][=C][C][=C][Ring1][=Branch1][=C][S][Ring2][Ring1][Ring1][C][C][Ring2][Ring1][=Branch2] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [O][=C][Branch1][S][N][/N][=C][/C][=C][C][=C][C][Branch1][C][O][=C][Ring1][#Branch1][O][C][=C][C][=C][Branch1][=C][C][=N][C][=C][C][=C][C][=C][Ring1][=Branch1][S][Ring1][=Branch2][C][=C][Ring1][#C] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [N][S][=Branch1][C][=O][=Branch1][C][=O][C][=C][C][Branch1][=Branch1][N+1][=Branch1][C][=O][O-1][=C][Branch1][C][Cl][S][Ring1][=Branch2] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [O][=C][Branch2][Ring1][Ring1][C][=N][N][=C][Branch1][#Branch2][N][C@H1][C][C][N][C][Ring1][Branch1][=O][O][Ring1][N][C][=N][C][=C][C][=C][Branch1][N][C][=C][C][=C][Branch1][C][F][C][=C][Ring1][#Branch1][C][=C][Ring1][=N][S][Ring1][S] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [N][C][=N][C][=C][Branch2][Ring1][Branch1][C][=N][N][Ring1][Branch1][C][O][C][C][C][C][C][P][=Branch1][C][=O][Branch1][C][O][O][C][=Branch1][C][=O][NH1][Ring2][Ring1][Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][O][C][=C][C][=C][C][Branch2][Ring2][=Branch1][N][C][=N][C][=C][C][Branch2][Ring1][Ring2][N][C][=N][N][=C][Branch1][Ring1][O][C][C][=C][C][=C][C][=C][Ring1][N][Ring1][=Branch1][=C][C][=C][Ring2][Ring1][Ring1][Ring2][Ring1][=Branch1][=C][Ring2][Ring1][N] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][C][O][C][=C][C][=C][Branch2][Ring1][O][C][#C][C][=C][C][=C][Branch1][#C][C][Branch1][C][C][Branch1][C][C][C][N][C][Branch1][C][C][=O][C][=C][Ring1][=C][C][=C][Ring2][Ring1][=Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [O][=S][=Branch1][C][=O][Branch1][N][C][=C][C][=C][Branch1][C][Cl][C][=C][Ring1][#Branch1][N][C][=C][Branch1][=Branch2][C][C@@H1][C][C][C][N][Ring1][Branch1][C][=C][C][=C][C][=C][Ring1][=Branch1][Ring1][#C] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [O][=C][Branch1][C][O][C][=Branch1][C][=O][N][C][Branch1][#Branch2][C][C][=C][C][=C][C][=C][Ring1][=Branch1][C][=Branch1][C][=O][O] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][=C][Branch1][=Branch1][C][=Branch1][C][=O][O][O][C][=C][Ring1][Branch2][C][=Branch1][C][=O][C][S][C][Ring1][#Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C@H1][Branch2][Ring2][Branch2][N][C][=N][C][=C][Branch1][=N][/C][=C][\\N][C][=Branch1][C][=O][N][C][Ring1][=Branch1][=O][C][=N][N][Ring1][=N][C][Branch1][#Branch1][N][C][C][C][Ring1][Ring1][=C][Ring2][Ring1][Branch1][C][#N][C][C][C][Ring1][Ring1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][=N][N][=C][Branch2][Ring1][N][C][Branch1][#Branch1][C][C][=Branch1][C][=O][O][C][=C][C][=C][C][Branch1][=Branch2][C][=C][C][=C][N][=C][Ring1][=Branch1][=C][Ring1][N][O][Ring2][Ring1][=Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [N][C][=Branch1][C][=O][/C][=Branch2][Ring1][N][=C][/C][=C][Branch1][=Branch1][C][=Branch1][C][=O][O][NH1][C][=C][C][Branch1][C][Cl][=C][C][Branch1][C][Cl][=C][Ring1][=C][Ring1][Branch2][C][=C][C][=C][C][=C][Ring1][=Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][=C][C][=Branch1][C][=O][C][C][Branch1][C][C][Branch1][C][C][/C][Ring1][=Branch2][=C][/C][C@@H1][Branch1][C][C][O] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][C][C@H1][Branch1][P][N][C][=N][C][=C][C][=C][C][=C][Ring1][=Branch1][C][=C][Ring1][#Branch2][C][C][=C][C][=C][Branch1][=N][C][=Branch1][C][=O][N][C][C][C][=Branch1][C][=O][O][C][=C][Ring1][=C] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][C][=C][S][C][=Ring1][Branch1][C][=N][C][Branch1][C][O][=C][Branch1][C][O][C][Branch1][=Branch1][C][=Branch1][C][=O][O][=N][Ring1][O] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][=C][C][=C][Branch2][Ring1][S][C][Branch2][Ring1][Ring2][C][C][C][=N][C][Branch1][=Branch2][C][=C][C][=C][C][=N][Ring1][=Branch1][=N][O][Ring1][O][C][C][C][C][C][Ring2][Ring1][Ring1][C][=C][Ring2][Ring1][=Branch2] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C@H1][C][O][C][=C][C][Branch2][Ring1][Branch1][N][C][C][N][Branch1][O][C][C][=C][C][=C][C][=C][Ring1][=Branch1][F][C][C][Ring1][=C][=C][C][=C][Ring2][Ring1][Ring2][S][=Branch1][C][=O][=Branch1][C][=O][N][Ring2][Ring1][O] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [O][=C][C][=C][C][=C][Branch1][#C][C][=C][Ring1][=Branch1][O][C][C][C][C][N][Ring1][=N][Ring1][Branch1][C][C][C][O][Ring1][=C] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][O][C][=C][C][=C][C][Branch1][Ring1][O][C][=C][Ring1][Branch2][C][=Branch1][C][=O][/C][=C][/C][=C][Branch1][Ring1][O][C][C][Branch1][C][F][=C][Branch1][Ring1][O][C][C][Branch1][C][F][=C][Ring1][N][O][C] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [O][=S][=Branch1][C][=O][Branch1][#C][C][C][C][C][C][C][C][=C][C][=C][C][=C][Ring1][=Branch1][C][=N][N][=C][Branch1][=Branch2][C][=C][C][=C][C][=N][Ring1][=Branch1][O][Ring1][O] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [O][=C][Branch1][Branch2][C][=C][C][=C][S][Ring1][Branch1][N][C][C][C][=C][C][Branch1][=N][O][C][C][C][N][C][C][C][C][C][Ring1][=Branch1][=C][C][=C][Ring1][S][C][Ring2][Ring1][Ring2] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [O][=C][Branch2][Ring1][C][N][C][=Branch1][C][=O][C][=C][Branch1][C][F][C][=C][C][=C][Ring1][#Branch1][Cl][C][S][N][=N][C][=Ring1][Branch1][C][C][C][Ring1][Ring1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][Branch1][C][C][C][C][N][Branch2][Ring2][Branch1][C][=C][C][=C][C][Branch2][Ring1][=Branch2][N][C][=C][C][Branch1][=Branch2][C][=C][C][=C][C][=C][Ring1][=Branch1][=N][N][C][=C][N][=C][Ring1][#C][Ring1][Branch1][=N][Ring2][Ring1][=Branch1][C][Ring2][Ring1][N] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][#C][C][S][C][=N][C][S][C][Branch1][C][C][=C][Branch1][C][C][C][=Ring1][#Branch1][C][=Branch1][C][=O][N][Ring1][N][C][=C][C][=C][C][=Branch1][Ring2][=C][Ring1][=Branch1][O][C][O][Ring1][=Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][=C][C][=C][C][=Branch1][Ring2][=C][Ring1][=Branch1][N][=C][Branch1][=Branch1][C][C][C][Ring1][Ring1][C][=C][N][=C][N][Ring1][Branch1][Ring1][=N] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [N][C][=N][C@@][Branch2][Ring1][N][C][=C][C][Branch1][P][N][C][=Branch1][C][=O][C][=C][N][=C][Branch1][C][Cl][C][=N][Ring1][#Branch1][=C][C][=C][Ring1][S][F][Branch1][=Branch1][C][Branch1][C][F][F][C][O][C][Ring2][Ring1][#Branch2] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][N][S][=Branch1][C][=O][=Branch1][C][=O][C][=C][C][=C][Branch1][Ring1][S][C][C][Branch1][#C][N][C][=N][C][=N][C][NH1][C][=C][C][Ring1][=Branch2][=Ring1][Branch1][=C][Ring2][Ring1][C] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][N][Branch1][C][C][C][=C][C][=C][Branch2][Ring1][#C][C][=C][C][=C][Branch2][Ring1][Ring2][C][C][O][C][Branch1][O][C][=C][C][=Branch1][C][=O][C][=C][Ring1][#Branch1][O][Ring1][O][C][=C][Ring1][P][C][=C][Ring2][Ring1][#Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][Branch1][C][C][N][C][=N][C][=C][C][=Branch1][Ring2][=N][Ring1][=Branch1][C][N][Branch2][Ring1][P][C][=Branch1][C][=O][N][C@H1][Branch1][O][C][N][C][=N][C][=C][C][=N][Ring1][=Branch1][C][=C][C][=C][Branch1][C][F][C][Branch1][C][Cl][=C][Ring1][Branch2][C][C][Ring2][Ring1][O] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [N][#C][C][=C][Branch1][C][O][N][=C][C][=C][C][Branch2][Ring1][Ring2][C][=C][C][=C][Branch1][#Branch2][C][=C][C][=C][C][=C][Ring1][=Branch1][O][C][=C][Ring1][=N][=C][C][Ring2][Ring1][Ring1][=C][Ring2][Ring1][Branch2][O] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][O][C][=C][C][=C][Branch1][Branch1][C][C][C][O][C][Branch2][Ring2][=Branch1][N][C][=N][C][=C][C][=C][C][=C][Ring1][=Branch1][N][=C][Ring1][#Branch2][N][S][=Branch1][C][=O][=Branch1][C][=O][C][=C][N][Branch1][C][C][C][Branch1][Ring1][C][O][=N][Ring1][Branch2][=C][Ring2][Ring1][P] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][C][O][C][=C][C][Branch1][=N][C][=Branch1][C][=O][N][C][C][O][C][C][Ring1][=Branch1][=N][N][Ring1][=N][C][=C][C][=C][C][=C][Ring1][=Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][=N][N][Branch2][Ring1][N][C][C][=C][C][=C][Branch1][P][N][C][=Branch1][C][=O][C][=C][C][=C][Branch1][C][Cl][C][=C][Ring1][#Branch1][C][=C][Ring1][S][C][Branch1][C][C][=C][Ring2][Ring1][#Branch1][C][C][=Branch1][C][=O][O] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][N][Branch1][Ring2][C][C][N][C][=Branch1][C][=O][C][=C][C][=C][C][=Branch1][Ring2][=C][Ring1][=Branch1][C][C][C][N][Ring1][#Branch1][C][=Branch1][C][=O][C][=C][C][=C][Branch1][C][O][C][=C][Ring1][#Branch1][O] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [O][=C][Branch1][C][O][C][=N][C][=C][C][=C][C][=C][Ring1][=Branch1][N][Branch2][Ring1][N][C@H1][C][C@H1][C][C][C@@H1][Branch1][Ring2][C][Ring1][#Branch1][N][Ring1][=Branch1][C][C][C][=C][C][=C][C][=C][Ring1][=Branch1][C][Ring1][=Branch2][C][Ring2][Ring1][O][=O] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [N][#C][C][=C][N][Branch1][S][C][=C][C][=C][Branch1][=Branch1][C][=Branch1][C][=O][O][C][=C][Ring1][=Branch2][C][=C][Ring1][=C][C][C][C][C][C][C][O][Ring1][Branch1] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][O][C][C][=C][C][=C][C][Branch2][Ring1][C][N][S][C][=C][C][Branch1][C][F][=C][C][=C][Ring1][#Branch1][C][Ring1][#Branch2][=O][=C][Ring1][P] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [O][=C][Branch1][#C][C][C][C][S][=Branch1][C][=O][=Branch1][C][=O][C][C][Ring1][Branch2][N][C][C][N][Branch1][=Branch2][C][=C][C][=C][C][=N][Ring1][=Branch1][C][C][Ring1][N][C][=C][C][=C][C][Branch1][C][F][=C][Ring1][#Branch1][F] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n",
      "Original: [C][C][C@@H1][Branch1][C][N][C][=C][C][=C][Branch1][C][Cl][C][Branch1][=N][C][=Branch1][C][=O][C][=C][C][=C][N][=C][Ring1][=Branch1][=C][Ring1][#C][F] \n",
      "\n",
      "Decoded:  [=C][=C]\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# set largers value to 1 and others to 0\n",
    "decoded_indices = torch.argmax(decoded.cpu(), dim=2)\n",
    "decoded_indices = decoded_indices.numpy()\n",
    "selfies_out = []\n",
    "for i, original in zip(decoded_indices, y):\n",
    "    vectorized = []\n",
    "    #print(f'Decoded: {i}')\n",
    "    #convert to one-hot\n",
    "    for number in i:\n",
    "        v = np.zeros(42)\n",
    "        v[number] = 1\n",
    "        vectorized.append(v)\n",
    "    selfies_out.append(vectorizer.devectorize(vectorized))\n",
    "    print(f'Original: {vectorizer.devectorize(original.cpu().numpy())} \\n')\n",
    "    print(f'Decoded:  {vectorizer.devectorize(vectorized)}')\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1efcd595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELFIES [=C][=C]\n",
      "SMILES C=C\n",
      "Attribution:\n",
      "AttributionMap(index=0, token='C', attribution=[Attribution(index=0, token='[=C]')])\n",
      "AttributionMap(index=1, token='=', attribution=[Attribution(index=1, token='[=C]')])\n",
      "AttributionMap(index=2, token='C', attribution=[Attribution(index=1, token='[=C]')])\n"
     ]
    }
   ],
   "source": [
    "selfie = selfies_out[1]\n",
    "smile, attr = sf.decoder(selfie, attribute=True)\n",
    "print('SELFIES', selfie)\n",
    "print('SMILES', smile)\n",
    "print('Attribution:')\n",
    "for smiles_token in attr:\n",
    "    print(smiles_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e56cf5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAIAAAD2HxkiAAADzElEQVR4nO3Ysa0iMRhG0WX1qoGM6YISaIMy6MIBJVAGIe14O3hyMrrS7DnxH3zJlSWf5px/gM7fegD870QIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEULsZ/Hu+/2+3+9dp8CR3G638/m8dDrXjDF23gyHMsZYjGv1JbxcLo/HY9fRcCSXy2Xx8jTn3HUK8DsfMxATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBD7Wbz7fD6v12vXKXAk9/v9er0unc41Y4ydN8OhjDEW41p9Cbdtez6fu46GI9m2bfHyNOfcdQrwOx8zEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAgxEUJMhBATIcRECDERQkyEEBMhxEQIMRFCTIQQEyHERAixf1fpv3vm2LchAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=300x300>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "m = Chem.MolFromSmiles(smile)\n",
    "img = Draw.MolToImage(m)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11c29f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
