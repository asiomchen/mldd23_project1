{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16dfcb1",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3fbcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selfies as sf\n",
    "data = pd.read_parquet('./GRU_data/combined_selfies.parquet')\n",
    "alphabet = sf.get_alphabet_from_selfies(data.selfies)\n",
    "#alphabet.add(\"[nop]\") # [nop] is a special padding symbol\n",
    "alphabet.add(\"[start]\")\n",
    "alphabet.add(\"[end]\")\n",
    "alphabet.add(\"[nop]\")\n",
    "alphabet = list(sorted(alphabet))\n",
    "pad_to_len = max(sf.len_selfies(s) for s in data.selfies) + 10\n",
    "print(\"Pad to len:\", pad_to_len)\n",
    "symbol_to_idx = {s: i for i, s in enumerate(alphabet)}\n",
    "idx2char = {i: s for i, s in enumerate(alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class SELFIESVectorizer:\n",
    "    def __init__(self, alphabet, pad_to_len=None):\n",
    "        self.alphabet = alphabet\n",
    "        self.char2idx = {s: i for i, s in enumerate(alphabet)}\n",
    "        self.idx2char = {i: s for i, s in enumerate(alphabet)}\n",
    "        self.pad_to_len = pad_to_len\n",
    "    def vectorize(self, selfie, no_special=False):\n",
    "        ''' Vectorize a list of SMILES strings to a numpy array of shape (len(smiles), embed, len(charset))'''\n",
    "        if no_special:\n",
    "            splited = self.split_selfi(selfie)\n",
    "        elif self.pad_to_len is None:\n",
    "            splited = ['[start]'] + self.split_selfi(selfie) + ['[end]']\n",
    "        else:\n",
    "            splited = ['[start]'] + self.split_selfi(selfie) + ['[end]'] + ['[nop]'] * (self.pad_to_len - len(self.split_selfi(selfie)) - 2)\n",
    "        X = np.zeros((len(splited), len(self.alphabet)))\n",
    "        for i in range(len(splited)):\n",
    "            X[i, self.char2idx[splited[i]]] = 1\n",
    "        return X\n",
    "    def devectorize(self, ohe, remove_special=False):\n",
    "        ''' Devectorize a numpy array of shape (len(smiles), embed, len(charset)) to a list of SMILES strings'''\n",
    "        selfie_str = ''\n",
    "        for j in range(ohe.shape[0]):\n",
    "            idx = np.argmax(ohe[j, :])\n",
    "            if remove_special and (self.idx2char[idx] == '[start]' or self.idx2char[idx] == '[end]'):\n",
    "                continue\n",
    "            selfie_str += self.idx2char[idx]\n",
    "        return selfie_str\n",
    "    def idxize(self, selfie, no_special=False):\n",
    "        if no_special:\n",
    "            splited = self.split_selfi(selfie)\n",
    "        else:\n",
    "            splited = ['[start]'] + self.split_selfi(selfie) + ['[end]'] + ['[nop]'] * (self.pad_to_len - len(self.split_selfi(selfie)) - 2)\n",
    "        return np.array([self.char2idx[s] for s in splited])\n",
    "    def deidxize(self, idx):\n",
    "        return \"\".join([self.idx2char[i] for i in idx])\n",
    "    def split_selfi(self, selfie):\n",
    "        pattern = r'(\\[[^\\[\\]]*\\])'\n",
    "        return re.findall(pattern, selfie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70924816",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = SELFIESVectorizer(alphabet, pad_to_len=pad_to_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GRUDatasetv2(Dataset):\n",
    "    def __init__(self, smiles_fp, selfies, vectorizer):\n",
    "        self.smiles_fp = pd.read_parquet(smiles_fp)\n",
    "        self.selfies = pd.read_parquet(selfies)\n",
    "        # self.X = self.prepare_X(self.smiles_fp)\n",
    "        # self.X = np.array([self.reconstruct_fp(fp) for fp in self.X])\n",
    "        self.selfies= self.prepare_y(self.selfies)\n",
    "        self.vectorizer = vectorizer\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_fp)\n",
    "    def __getitem__(self, idx):\n",
    "        raw_selfie = self.selfies[idx][0]\n",
    "        vectorized_selfie = self.vectorizer.idxize(raw_selfie)\n",
    "        # esentially, we want to predict the next symbol in the SELFIE and offset the target by one makes teaching forcing implicit\n",
    "        vectorized_selfie = vectorized_selfie\n",
    "        raw_X = self.smiles_fp.fps[idx]\n",
    "        X = np.array(raw_X, dtype=int)\n",
    "        X_reconstructed = self.reconstruct_fp(X)\n",
    "\n",
    "        return torch.from_numpy(X_reconstructed).float(), torch.from_numpy(vectorized_selfie).long()\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_X(smiles_fp):\n",
    "        fps = smiles_fp.fps.apply(eval).apply(lambda x: np.array(x, dtype=int))\n",
    "        return fps\n",
    "    @staticmethod\n",
    "    def prepare_y(selfies):\n",
    "        return selfies.values\n",
    "    @staticmethod\n",
    "    def reconstruct_fp(fp, length=4860):\n",
    "        fp_rec = np.zeros(length)\n",
    "        fp_rec[fp] = 1\n",
    "        return fp_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062168b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GRUDatasetv2('GRU_data/combined_klek.parquet', 'GRU_data/combined_selfies.parquet', vectorizer)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Test size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f39daf",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNet(nn.Module):\n",
    "    def __init__(self, fp_size, encoding_size):\n",
    "        super(EncoderNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(fp_size, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, encoding_size)\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.relu(self.fc5(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderNet(nn.Module):\n",
    "    def __init__(self, dictionary_size, emb_size, hidden_size, num_layers, drop_prob):\n",
    "        super(DecoderNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        # embedding layer [batch_size, seq_len] -> [batch_size, seq_len, emb_size]\n",
    "        self.embedding = nn.Embedding(dictionary_size, emb_size)\n",
    "        # gru layer [batch_size, seq_len, emb_size] -> [batch_size, seq_len, hidden_size]\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        # fully connected layer [batch_size, seq_len, hidden_size] -> [batch_size, seq_len, dictionary_size]\n",
    "        self.fc = nn.Linear(hidden_size, dictionary_size)\n",
    "        self.max_len = vectorizer.pad_to_len\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emdedded = self.embedding(input)\n",
    "        prediction, hidden = self.gru(emdedded, hidden)\n",
    "        prediction = self.fc(prediction)\n",
    "        return prediction, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, encoded):\n",
    "        return encoded.unsqueeze(0).repeat(self.num_layers, 1, 1).to(device)\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=4860, \n",
    "                 encoding_size=256, \n",
    "                 dictionary_size=len(alphabet), \n",
    "                 emb_size=256, \n",
    "                 hidden_size=256, \n",
    "                 num_layers=2, \n",
    "                 teacher_forcing_ratio=0.5,\n",
    "                 drop_prob=0.2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = EncoderNet(input_size, encoding_size)\n",
    "        self.decoder = DecoderNet(dictionary_size, emb_size, hidden_size, num_layers, drop_prob)\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "    def forward(self, src,trg, teacher_forcing_ratio=None):\n",
    "        teacher_forcing_ratio = self.teacher_forcing_ratio if teacher_forcing_ratio is None else teacher_forcing_ratio\n",
    "        # if in evaluation mode we don't use teacher forcing\n",
    "        if not self.training:\n",
    "            teacher_forcing_ratio = 0\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "        outputs = []\n",
    "        hidden = self.encoder(src)\n",
    "        hidden = hidden.unsqueeze(0).repeat(self.decoder.num_layers, 1, 1)\n",
    "        input = trg[:, 0].unsqueeze(1)\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs.append(output)\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = torch.argmax(output, dim=2)\n",
    "            input = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f697df",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce412c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "\n",
    "def pretty_str_compare(str, str_pred, color=True):\n",
    "    if len(str_pred) > len(str):\n",
    "        str += '-' * (len(str_pred) - len(str))\n",
    "    elif len(str_pred) < len(str):\n",
    "        str_pred += '-' * (len(str) - len(str_pred))\n",
    "    print(f'Target:  {str}', end='\\n')\n",
    "    # print difference in red color\n",
    "    print('Output:  ', end='')\n",
    "    for act, pred in zip(str, str_pred):\n",
    "        if act != pred:\n",
    "            if color:\n",
    "                print('\\033[91m', end='')\n",
    "            print(pred, end='')\n",
    "            if color:\n",
    "                print('\\033[0m', end='')\n",
    "        else:\n",
    "            print(pred, end='')\n",
    "\n",
    "def train(autoencoder, dataloader, val_dataloader, num_epochs=10, device=device):\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    teacher_forcing_ratio = 0.5\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vectorizer.char2idx['[nop]'])\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "    autoencoder.train()\n",
    "    config = dict()\n",
    "    config['learning_rate'] = learning_rate\n",
    "    config['teacher_forcing_ratio'] = teacher_forcing_ratio\n",
    "    config['criterion'] = criterion\n",
    "    config['optimizer'] = optimizer\n",
    "    config['num_epochs'] = num_epochs\n",
    "    config['Trainable parameters'] = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
    "    config['Embedding size'] = autoencoder.decoder.embedding.embedding_dim\n",
    "    config['Hidden size'] = autoencoder.decoder.hidden_size\n",
    "    config['Number of layers'] = autoencoder.decoder.num_layers\n",
    "    config['Dropout'] = autoencoder.decoder.gru.dropout\n",
    "    config['Batch size'] = batch_size\n",
    "    config['Dataset size'] = len(dataloader.dataset) * batch_size\n",
    "\n",
    "    \n",
    "\n",
    "    wandb.init(project=\"selfie-autoencoder_v2\", config=config)\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (src, trg) in enumerate(tqdm(dataloader)):\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = autoencoder(src, trg, teacher_forcing_ratio)\n",
    "            trg = trg[:, 1:]\n",
    "            # print(f'Output shape: {output.shape}')\n",
    "            # print(f'Target shape: {trg.shape}')\n",
    "            # print('Target:', trg)\n",
    "            #replace 1 and 2 dimension\n",
    "            output = output.permute(0, 2, 1)\n",
    "            # print(f'Output shape after view: {output.shape}')\n",
    "            # print(f'Target shape after view: {trg.shape}')\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        if epoch % 2 == 0:\n",
    "            autoencoder.eval()\n",
    "            with torch.no_grad():\n",
    "                visual_examples_src, visual_examples_trg = next(iter(dataloader))\n",
    "                visual_examples_src = visual_examples_src.to(device)[:5]\n",
    "                visual_examples_trg = visual_examples_trg.to(device)[:5]\n",
    "                visual_examples_outputs = autoencoder(visual_examples_src, visual_examples_trg, teacher_forcing_ratio)\n",
    "                visual_examples_outputs = visual_examples_outputs.permute(0, 2, 1)\n",
    "                visual_examples_outputs = torch.argmax(visual_examples_outputs, dim=1)\n",
    "                visual_examples_outputs = visual_examples_outputs.cpu().numpy()\n",
    "                visual_examples_outputs = np.array([vectorizer.deidxize(o) for o in visual_examples_outputs])\n",
    "                visual_examples_trg = visual_examples_trg.cpu().numpy()\n",
    "                visual_examples_trg = np.array([vectorizer.deidxize(o) for o in visual_examples_trg])\n",
    "                print(f'Epoch {epoch} loss: {epoch_loss / len(dataloader)}')\n",
    "                for i in range(len(visual_examples_outputs)):\n",
    "                    pretty_str_compare(visual_examples_trg[i][7:], visual_examples_outputs[i])\n",
    "                    print('\\n-------------------\\n')\n",
    "        # evaluate on validation set\n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batch_idx, (src, trg) in enumerate(tqdm(val_dataloader)):\n",
    "                src = src.to(device)\n",
    "                trg = trg.to(device)\n",
    "                output = autoencoder(src, trg, teacher_forcing_ratio)\n",
    "                trg = trg[:, 1:]\n",
    "                output = output.permute(0, 2, 1)\n",
    "                loss = criterion(output, trg)\n",
    "                val_loss += loss.item()\n",
    "            wandb.log({'val_loss': val_loss / len(val_dataloader)})\n",
    "            autoencoder.train()\n",
    "        print(f'Epoch {epoch} loss: {epoch_loss / len(dataloader)}')\n",
    "        wandb.log({'loss': epoch_loss / len(dataloader)})\n",
    "    wandb.finish()\n",
    "    return autoencoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db232dc2",
   "metadata": {},
   "source": [
    "## Sweep config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eabc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    }\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "parameters_dict = {\n",
    "    'encoding_size': { 'values': [32, 64, 128, 256, 512, 1024] },\n",
    "    'emb_size': { 'values': [32, 64, 128, 256, 512, 1024] }, \n",
    "    'num_layers': { 'values': [1, 2, 3, 4, 5] },\n",
    "    'teacher_forcing_ratio': { 'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] }, # 'min': 0.0, 'max': 0.5, 'step': 0.1 },\n",
    "    'drop_prob': { 'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5] },\n",
    "    'epochs': { 'value': 20}\n",
    "    }\n",
    "\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "parameters_dict.update({\n",
    "    'learning_rate': {\n",
    "        # a flat distribution between 0 and 0.01\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.0,\n",
    "        'max': 0.01\n",
    "      },\n",
    "    'batch_size': {\n",
    "        'value': 256\n",
    "      }\n",
    "    })\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ad4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep id is created only once, then string is used to run the sweep\n",
    "#sweep_id = wandb.sweep(sweep_config, project=\"selfie-autoencoder_v2\")\n",
    "sweep_id = 'jjgazi3d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_hyper(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # Config is a variable that holds and saves hyperparameters and inputs\n",
    "        config = wandb.config\n",
    "        # Define model, optimizer, and loss function\n",
    "        autoencoder = Autoencoder(encoding_size=config.encoding_size, \n",
    "                                  emb_size=config.emb_size, \n",
    "                                  hidden_size=config.encoding_size,\n",
    "                                  num_layers=config.num_layers, \n",
    "                                    teacher_forcing_ratio=config.teacher_forcing_ratio,\n",
    "                                  drop_prob=config.drop_prob).to(device)\n",
    "        dataloader = DataLoader(train_dataset, shuffle=True, batch_size=config.batch_size, drop_last=True)\n",
    "        val_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=config.batch_size, drop_last=True)\n",
    "        for i in tqdm(range(config.epochs)):\n",
    "            avg_loss = train_epoch(autoencoder, dataloader, config.learning_rate)\n",
    "            val_loss, table = evaluate(autoencoder, val_dataloader)\n",
    "            wandb.log({'epoch': i, 'loss': avg_loss})\n",
    "            wandb.log({'epoch': i, 'val_loss': val_loss})\n",
    "            wandb.log({'epoch': i, 'table': table})\n",
    "\n",
    "\n",
    "def train_epoch(autoencoder, dataloader, learning_rate):\n",
    "    autoencoder.train()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vectorizer.char2idx['[nop]'])\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (src, trg) in enumerate(tqdm(dataloader)):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = autoencoder(src, trg)\n",
    "        trg = trg[:, 1:]\n",
    "        output = output.permute(0, 2, 1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        wandb.log({'batch_loss': loss.item()})\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(autoencoder, dataloader):\n",
    "    autoencoder.eval()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vectorizer.char2idx['[nop]'])\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (src, trg) in enumerate(tqdm(dataloader)):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        output = autoencoder(src, trg)\n",
    "        trg = trg[:, 1:]\n",
    "        output = output.permute(0, 2, 1)\n",
    "        loss = criterion(output, trg)\n",
    "        epoch_loss += loss.item()\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    with torch.no_grad():\n",
    "                visual_examples_src, visual_examples_trg = next(iter(dataloader))\n",
    "                visual_examples_src = visual_examples_src.to(device)[:5]\n",
    "                visual_examples_trg = visual_examples_trg.to(device)[:5]\n",
    "                visual_examples_outputs = autoencoder(visual_examples_src, visual_examples_trg)\n",
    "                visual_examples_outputs = visual_examples_outputs.permute(0, 2, 1)\n",
    "                visual_examples_outputs = torch.argmax(visual_examples_outputs, dim=1)\n",
    "                visual_examples_outputs = visual_examples_outputs.cpu().numpy()\n",
    "                visual_examples_outputs = np.array([vectorizer.deidxize(o) for o in visual_examples_outputs])\n",
    "                visual_examples_trg = visual_examples_trg.cpu().numpy()\n",
    "                visual_examples_trg = np.array([vectorizer.deidxize(o) for o in visual_examples_trg])\n",
    "                #log visual examples as table to wandb\n",
    "                data = [[src, trg] for src, trg in zip(visual_examples_trg, visual_examples_outputs)]\n",
    "                table = wandb.Table(data=data, columns = [\"Target\", \"Output\"])\n",
    "                     \n",
    "    return avg_loss, table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id,project=\"selfie-autoencoder_v2\", function=train_hyper, count=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
