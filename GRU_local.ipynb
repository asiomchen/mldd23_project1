{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef6f3f5",
   "metadata": {
    "id": "6ef6f3f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/hubert/.netrc\r\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from gru.example_printer import ExamplePrinter\n",
    "from gru.dataset import GRUDataset\n",
    "from gru.gru_v3 import EncoderNet, DecoderNet, EncoderDecoder\n",
    "from vectorizer import SELFIESVectorizer, determine_alphabet\n",
    "from gru.cce import CCE, ConsciousCrossEntropy\n",
    "from split import scaffold_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# weights and biases\n",
    "!wandb login 505ce3ad45fdf9309c3d8ec1d9764262ae6929c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "N3BCODbh2PhC",
   "metadata": {
    "id": "N3BCODbh2PhC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16dfcb1",
   "metadata": {
    "id": "f16dfcb1"
   },
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ce528d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1093607\n",
      "Val size: 121524\n"
     ]
    }
   ],
   "source": [
    "alphabet = pd.read_csv('./GRU_data/alphabet.txt', header=None).values.flatten()\n",
    "vectorizer = SELFIESVectorizer(alphabet, pad_to_len=128)\n",
    "\n",
    "#data_path = './GRU_data/combned_dataset.parquet'\n",
    "#dataset = pd.read_parquet(data_path)\n",
    "\n",
    "#train_size = 0.9\n",
    "\n",
    "#train_df, val_df = scaffold_split(dataset, train_size)\n",
    "\n",
    "train_df = pd.read_parquet('./models/v3-revisited/train_dataset.parquet')\n",
    "val_df = pd.read_parquet('./models/v3-revisited/val_dataset.parquet')\n",
    "\n",
    "train_dataset = GRUDataset(train_df, vectorizer)\n",
    "val_dataset = GRUDataset(val_df, vectorizer)\n",
    "\n",
    "#print(\"Dataset size:\", len(dataset))\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Val size:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da4f014e",
   "metadata": {
    "id": "da4f014e"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f39daf",
   "metadata": {
    "id": "02f39daf"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f598e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'v3-revisited'\n",
    "\n",
    "# Set hyperparameters\n",
    "encoding_size = 512\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "learn_rate = 0.0003\n",
    "dropout = 0 # dropout must be equal 0 if num_layers = 1\n",
    "teacher_ratio = 0.5\n",
    "\n",
    "# Init model\n",
    "model = EncoderDecoder(\n",
    "    fp_size=4860,\n",
    "    encoding_size=encoding_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    teacher_ratio = teacher_ratio).to(device)\n",
    "\n",
    "#! mkdir ./models/{run_name}\n",
    "#train_df.to_parquet(f'./models/{run_name}/train_dataset.parquet', index=False)\n",
    "#val_df.to_parquet(f'./models/{run_name}/val_dataset.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4d085d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./GRU_data/params/v3_w-teacher-w-enumeration-0.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f697df",
   "metadata": {
    "id": "c3f697df"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7b00cca",
   "metadata": {
    "id": "f7b00cca"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, train_loader, val_loader, vectorizer, device):\n",
    "\n",
    "    EPOCHS = 25\n",
    "\n",
    "    # Define dataframe for training progess display\n",
    "    epochs_range = range(1,EPOCHS+1)\n",
    "    metrics = pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss']);\n",
    "    metrics['epoch'] = epochs_range\n",
    "    \n",
    "    # Init example printer\n",
    "    printer = ExamplePrinter(val_loader, 25)\n",
    "\n",
    "    # Define pyplot for plotting metrics\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(7, 3), layout=\"constrained\")\n",
    "    dh = display(fig, display_id=True)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    criterion = CCE()\n",
    "\n",
    "    # wandb config and init\n",
    "    config = dict()\n",
    "    config['learning rate'] = learn_rate\n",
    "    config['encoding size'] = model.encoding_size\n",
    "    config['criterion'] = criterion\n",
    "    config['optimizer'] = optimizer\n",
    "    config['num epochs'] = EPOCHS\n",
    "    config['Trainable parameters'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    config['hidden size'] = model.hidden_size\n",
    "    config['Number of layers'] = num_layers\n",
    "    config['Dropout'] = model.decoder.dropout\n",
    "    config['Batch size'] = batch_size\n",
    "    config['teacher_ratio'] = teacher_ratio\n",
    "    wandb.init(project=\"encoded-token-concat\", config=config)\n",
    "\n",
    "    print(\"Starting Training of GRU\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    # Start training loop\n",
    "    for epoch in epochs_range:\n",
    "        print(f'Epoch: {epoch}')\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for batch_idx, (X,y) in enumerate(tqdm(train_loader)):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X, y, teacher_forcing=True).to(device)\n",
    "            loss = criterion(y, output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # calculate loss and log to wandb\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        val_loss = evaluate(model, val_loader)\n",
    "        metrics_dict = {'epoch': epoch,\n",
    "                        'train_loss': avg_loss,\n",
    "                        'val_loss': val_loss}\n",
    "        wandb.log(metrics_dict)\n",
    "\n",
    "        # Update metrics df\n",
    "        metrics.loc[len(metrics)] = metrics_dict\n",
    "\n",
    "        # Display metrics\n",
    "        ax[0].clear()\n",
    "        ax[0].plot(metrics.epoch, metrics.train_loss)\n",
    "        ax[0].set_title('training loss')\n",
    "        ax[0].set_xlabel('epoch')\n",
    "        ax[0].set_ylabel('CrossEntropy')\n",
    "        ax[1].clear()\n",
    "        ax[1].plot(metrics.epoch, metrics.val_loss)\n",
    "        ax[1].set_title('validation loss')\n",
    "        ax[1].set_xlabel('epoch')\n",
    "        ax[1].set_ylabel('CrossEntropy')\n",
    "        dh.update(fig)\n",
    "        \n",
    "        try:\n",
    "            new_samples = printer(model)\n",
    "            samples.append(new_samples)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        save_path = f\"./models/{run_name}/model_epoch_{epoch}.pt\"\n",
    "        torch.save(model.state_dict(),save_path)\n",
    "        \n",
    "    plt.close()\n",
    "    wandb.finish()\n",
    "    return model, samples\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    criterion = ConsciousCrossEntropy()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (X, y) in enumerate(val_loader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(X, y, teacher_forcing=False).to(device)\n",
    "        loss = criterion(y, output)\n",
    "        epoch_loss += loss.item()\n",
    "    avg_loss = epoch_loss / len(val_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874519b",
   "metadata": {
    "id": "c874519b"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0839d044",
   "metadata": {
    "id": "0839d044"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:dg4uwgpk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_loss</td><td>0.37324</td></tr><tr><td>val_loss</td><td>0.80066</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerulean-feather-57</strong> at: <a href='https://wandb.ai/mldd23/encoded-token-concat/runs/dg4uwgpk' target=\"_blank\">https://wandb.ai/mldd23/encoded-token-concat/runs/dg4uwgpk</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230623_134746-dg4uwgpk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:dg4uwgpk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82aab91e427459eb49789deee45bf81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016672146416666086, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hubert/github/mldd23_project1/wandb/run-20230623_135223-nmho6fav</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mldd23/encoded-token-concat/runs/nmho6fav' target=\"_blank\">elated-fire-58</a></strong> to <a href='https://wandb.ai/mldd23/encoded-token-concat' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mldd23/encoded-token-concat' target=\"_blank\">https://wandb.ai/mldd23/encoded-token-concat</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mldd23/encoded-token-concat/runs/nmho6fav' target=\"_blank\">https://wandb.ai/mldd23/encoded-token-concat/runs/nmho6fav</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU\n",
      "Device: cuda\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 1/4271 [00:01<1:37:37,  1.37s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 1.95 GiB total capacity; 1.37 GiB already allocated; 10.62 MiB free; 1.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, samples \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_final.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[19], line 52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, vectorizer, device)\u001b[0m\n\u001b[1;32m     50\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 52\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y, output)\n\u001b[1;32m     54\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/mldd23/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/github/mldd23_project1/gru/gru_v3.py:76\u001b[0m, in \u001b[0;36mEncoderDecoder.forward\u001b[0;34m(self, x, y, teacher_forcing)\u001b[0m\n\u001b[1;32m     74\u001b[0m decoded \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m128\u001b[39m):\n\u001b[0;32m---> 76\u001b[0m     out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(out))\n\u001b[1;32m     78\u001b[0m     decoded\u001b[38;5;241m.\u001b[39mappend(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/mldd23/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/github/mldd23_project1/gru/gru_v3.py:43\u001b[0m, in \u001b[0;36mDecoderNet.forward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, h):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m#x.shape = [batch_size, selfie_len, encoding_size] = [64, 128, 256]\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     out, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m#out.shape = [batch_size, selfie_len, hidden_size] = [64, 128, 256]\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m#h.shape = [num_layers, batch_size, hidden_size] = [1, 64, 256]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, h\n",
      "File \u001b[0;32m~/miniconda3/envs/mldd23/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mldd23/lib/python3.9/site-packages/torch/nn/modules/rnn.py:955\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    959\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 1.95 GiB total capacity; 1.37 GiB already allocated; 10.62 MiB free; 1.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, samples = train(model, train_loader, val_loader, vectorizer, device)\n",
    "\n",
    "# save model\n",
    "save_path = f\"./models/{run_name}/model_final.pt\"\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
