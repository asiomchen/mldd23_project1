{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef6f3f5",
   "metadata": {
    "id": "6ef6f3f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/hubert/.netrc\r\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from gru import example_printer\n",
    "from gru.dataset import GRUDataset\n",
    "from gru.gru_v3 import EncoderNet, DecoderNet, EncoderDecoder\n",
    "from vectorizer import SELFIESVectorizer, determine_alphabet\n",
    "from gru.cce import CCE, ConsciousCrossEntropy\n",
    "from split import scaffold_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# weights and biases\n",
    "!wandb login 505ce3ad45fdf9309c3d8ec1d9764262ae6929c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "N3BCODbh2PhC",
   "metadata": {
    "id": "N3BCODbh2PhC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16dfcb1",
   "metadata": {
    "id": "f16dfcb1"
   },
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce528d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1093607\n",
      "Val size: 121524\n"
     ]
    }
   ],
   "source": [
    "alphabet = pd.read_csv('./GRU_data/alphabet.txt', header=None).values.flatten()\n",
    "vectorizer = SELFIESVectorizer(alphabet, pad_to_len=128)\n",
    "\n",
    "#data_path = './GRU_data/combned_dataset.parquet'\n",
    "#dataset = pd.read_parquet(data_path)\n",
    "\n",
    "#train_size = 0.9\n",
    "\n",
    "#train_df, val_df = scaffold_split(dataset, train_size)\n",
    "\n",
    "train_df = pd.read_parquet('./models/v3-revisited/train_dataset.parquet')\n",
    "val_df = pd.read_parquet('./models/v3-revisited/val_dataset.parquet')\n",
    "\n",
    "train_dataset = GRUDataset(train_df, vectorizer)\n",
    "val_dataset = GRUDataset(val_df, vectorizer)\n",
    "\n",
    "#print(\"Dataset size:\", len(dataset))\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Val size:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4f014e",
   "metadata": {
    "id": "da4f014e"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f39daf",
   "metadata": {
    "id": "02f39daf"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f598e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'v3-revisited'\n",
    "\n",
    "# Set hyperparameters\n",
    "encoding_size = 512\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "learn_rate = 0.0003\n",
    "dropout = 0 # dropout must be equal 0 if num_layers = 1\n",
    "teacher_ratio = 0.5\n",
    "\n",
    "# Init model\n",
    "model = EncoderDecoder(\n",
    "    fp_size=4860,\n",
    "    encoding_size=encoding_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    teacher_ratio = teacher_ratio).to(device)\n",
    "\n",
    "#! mkdir ./models/{run_name}\n",
    "#train_df.to_parquet(f'./models/{run_name}/train_dataset.parquet', index=False)\n",
    "#val_df.to_parquet(f'./models/{run_name}/val_dataset.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d27e63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./GRU_data/params/v3_w-teacher-w-enumeration-0.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f697df",
   "metadata": {
    "id": "c3f697df"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b00cca",
   "metadata": {
    "id": "f7b00cca"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, train_loader, val_loader, vectorizer, device):\n",
    "\n",
    "    EPOCHS = 25\n",
    "\n",
    "    # Define dataframe for training progess display\n",
    "    epochs_range = range(1,EPOCHS+1)\n",
    "    metrics = pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss']);\n",
    "    metrics['epoch'] = epochs_range\n",
    "    \n",
    "    # Init example printer\n",
    "    printer = example_printer.ExamplePrinter(alphabet, val_loader, num_examples=25)\n",
    "\n",
    "    # Define pyplot for plotting metrics\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(7, 3), layout=\"constrained\")\n",
    "    dh = display(fig, display_id=True)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    criterion = CCE()\n",
    "\n",
    "    # wandb config and init\n",
    "    config = dict()\n",
    "    config['learning rate'] = learn_rate\n",
    "    config['encoding size'] = model.encoding_size\n",
    "    config['criterion'] = criterion\n",
    "    config['optimizer'] = optimizer\n",
    "    config['num epochs'] = EPOCHS\n",
    "    config['Trainable parameters'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    config['hidden size'] = model.hidden_size\n",
    "    config['Number of layers'] = num_layers\n",
    "    config['Dropout'] = model.decoder.dropout\n",
    "    config['Batch size'] = batch_size\n",
    "    config['teacher_ratio'] = teacher_ratio\n",
    "    wandb.init(project=\"encoded-token-concat\", config=config)\n",
    "\n",
    "    print(\"Starting Training of GRU\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    # Start training loop\n",
    "    for epoch in epochs_range:\n",
    "        print(f'Epoch: {epoch}')\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for batch_idx, (X,y) in enumerate(tqdm(train_loader)):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X, y, teacher_forcing=True).to(device)\n",
    "            loss = criterion(y, output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # calculate loss and log to wandb\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        val_loss = evaluate(model, val_loader)\n",
    "        metrics_dict = {'epoch': epoch,\n",
    "                        'train_loss': avg_loss,\n",
    "                        'val_loss': val_loss}\n",
    "        wandb.log(metrics_dict)\n",
    "\n",
    "        # Update metrics df\n",
    "        metrics.loc[len(metrics)] = metrics_dict\n",
    "\n",
    "        # Display metrics\n",
    "        ax[0].clear()\n",
    "        ax[0].plot(metrics.epoch, metrics.train_loss)\n",
    "        ax[0].set_title('training loss')\n",
    "        ax[0].set_xlabel('epoch')\n",
    "        ax[0].set_ylabel('CrossEntropy')\n",
    "        ax[1].clear()\n",
    "        ax[1].plot(metrics.epoch, metrics.val_loss)\n",
    "        ax[1].set_title('validation loss')\n",
    "        ax[1].set_xlabel('epoch')\n",
    "        ax[1].set_ylabel('CrossEntropy')\n",
    "        dh.update(fig)\n",
    "        \n",
    "        new_samples = printer(model)\n",
    "        samples.append(new_samples)\n",
    "        \n",
    "        save_path = f\"./models/{run_name}/model_epoch_{epoch}.pt\"\n",
    "        torch.save(model.state_dict(),save_path)\n",
    "        \n",
    "    plt.close()\n",
    "    wandb.finish()\n",
    "    return model, samples\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    criterion = ConsciousCrossEntropy()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (X, y) in enumerate(val_loader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(X, y, teacher_forcing=False).to(device)\n",
    "        loss = criterion(y, output)\n",
    "        epoch_loss += loss.item()\n",
    "    avg_loss = epoch_loss / len(val_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874519b",
   "metadata": {
    "id": "c874519b"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0839d044",
   "metadata": {
    "id": "0839d044"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'num_examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, samples \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_final.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, vectorizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m epochs_range\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Init example printer\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m printer \u001b[38;5;241m=\u001b[39m \u001b[43mexample_printer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExamplePrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43malphabet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Define pyplot for plotting metrics\u001b[39;00m\n\u001b[1;32m     16\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m3\u001b[39m), layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstrained\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'num_examples'"
     ]
    }
   ],
   "source": [
    "model, samples = train(model, train_loader, val_loader, vectorizer, device)\n",
    "\n",
    "# save model\n",
    "save_path = f\"./models/{run_name}/model_final.pt\"\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef1e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
